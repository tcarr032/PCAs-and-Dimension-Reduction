{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial.transform import Rotation\n",
    "\n",
    "m = 60\n",
    "X = np.zeros((m, 3))  # initialize 3D dataset\n",
    "np.random.seed(42)\n",
    "angles = (np.random.rand(m) ** 3 + 0.5) * 2 * np.pi  # uneven distribution\n",
    "X[:, 0], X[:, 1] = np.cos(angles), np.sin(angles) * 0.5  # oval\n",
    "X += 0.28 * np.random.randn(m, 3)  # add more noise\n",
    "X = Rotation.from_rotvec([np.pi / 29, -np.pi / 20, np.pi / 4]).apply(X)\n",
    "X += [0.2, 0, 0.2]  # shift a bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c1: [0.67857588 0.70073508 0.22023881]\n",
      "c2: [-0.72817329  0.6811147   0.07646185]\n"
     ]
    }
   ],
   "source": [
    "X_centered = X - X.mean(axis=0)\n",
    "\n",
    "U, s, Vt = np.linalg.svd(X_centered)\n",
    "c1 = Vt[0]\n",
    "c2 = Vt[1]\n",
    "print(f'c1: {c1}')\n",
    "print(f'c2: {c2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c1 and c2 are unit vector for each of the pricpal components for the 3D dataset\n",
    "\n",
    "This can then be projected onto a hyperplane to tranform it to 2D space or d-space\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "old shape: (60, 3)\n",
      "new shape: (60, 2)\n"
     ]
    }
   ],
   "source": [
    "W2 = Vt[:2].T\n",
    "X2D = X_centered @ W2\n",
    "print(f'old shape: {X_centered.shape}')\n",
    "print(f'new shape: {X2D.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first 5 rows without sklearn:\n",
      " [[-0.87323119 -0.29459803]\n",
      " [ 0.14888518  0.51493557]\n",
      " [ 1.35121872 -0.39950155]\n",
      " [ 0.45436676 -0.1399845 ]\n",
      " [-0.73438909 -0.02289346]]\n",
      "first 5 rows with sklearn:\n",
      " [[-0.87323119  0.29459803]\n",
      " [ 0.14888518 -0.51493557]\n",
      " [ 1.35121872  0.39950155]\n",
      " [ 0.45436676  0.1399845 ]\n",
      " [-0.73438909  0.02289346]]\n"
     ]
    }
   ],
   "source": [
    "#With sklearn\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "print(f'first 5 rows without sklearn:\\n {X2D[:5,:]}')\n",
    "X2D = pca.fit_transform(X_centered)\n",
    "print(f'first 5 rows with sklearn:\\n {X2D[:5,:]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.67857588,  0.70073508,  0.22023881],\n",
       "       [ 0.72817329, -0.6811147 , -0.07646185]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.7578477 , 0.15186921])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "mnist = fetch_openml('mnist_784', as_frame=False)\n",
    "X_train, X_test = mnist.data[:60000], mnist.data[60_000:]\n",
    "y_train, y_test = mnist.target[:60000], mnist.target[60000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "154"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca = PCA()\n",
    "pca.fit(X_train)\n",
    "cumsum = np.cumsum(pca.explained_variance_ratio_)\n",
    "d = np.argmax(cumsum >= 0.95) + 1\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=0.95)\n",
    "X_reduced = pca.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "154"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.n_components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3,\n",
       "                   estimator=Pipeline(steps=[('pca', PCA(random_state=42)),\n",
       "                                             ('randomforestclassifier',\n",
       "                                              RandomForestClassifier(random_state=42))]),\n",
       "                   n_jobs=-1,\n",
       "                   param_distributions={'pca__n_components': array([10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26,\n",
       "       27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43,\n",
       "       44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57,...\n",
       "       414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426,\n",
       "       427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439,\n",
       "       440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452,\n",
       "       453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465,\n",
       "       466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478,\n",
       "       479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491,\n",
       "       492, 493, 494, 495, 496, 497, 498, 499])},\n",
       "                   random_state=42)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "clf = make_pipeline(PCA(random_state=42), RandomForestClassifier(random_state=42))\n",
    "\n",
    "params = {\n",
    "    \"pca__n_components\": np.arange(10,80),\n",
    "    \"randomforestclassifier__n_estimators\": np.arange(50,500)\n",
    "}\n",
    "\n",
    "rnd_search = RandomizedSearchCV(clf, params, cv=3, n_iter=10, n_jobs=-1, random_state=42)\n",
    "rnd_search.fit(X_train[:1000], y_train[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('pca', PCA(n_components=23, random_state=42)),\n",
      "                ('randomforestclassifier',\n",
      "                 RandomForestClassifier(n_estimators=465, random_state=42))])\n"
     ]
    }
   ],
   "source": [
    "print(rnd_search.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 123.93258864, -312.67426198,  -24.51405174, ...,  -62.00213296,\n",
       "          -8.8147422 ,  -66.93993166],\n",
       "       [1011.71837586, -294.85703831,  596.33956108, ...,  -24.52514836,\n",
       "          26.58534428,   16.99077095],\n",
       "       [ -51.84960804,  392.17315289, -188.50974941, ...,   -8.99144972,\n",
       "          -2.99473092,   56.93622984],\n",
       "       ...,\n",
       "       [-178.0534496 ,  160.0782111 , -257.61308227, ...,   35.30439525,\n",
       "          -2.75142691,   23.97581712],\n",
       "       [ 130.60607212,   -5.59193632,  513.85867376, ...,  -15.84132904,\n",
       "         -18.38612585,   39.40742042],\n",
       "       [-173.43595246,  -24.71880228,  556.01889398, ...,   29.62816702,\n",
       "         -52.61652274,   27.99524134]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnd_pca = PCA(n_components=154, svd_solver='randomized', random_state=42)\n",
    "X_reduced = rnd_pca.fit_transform(X_train)\n",
    "X_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 123.93240015,  312.67412765,   24.51396855, ...,   55.02414967,\n",
       "         -18.82319306,   57.12605157],\n",
       "       [1011.71883902,  294.85791533, -596.3396284 , ...,   40.79115354,\n",
       "         -28.52753525,  -32.93944347],\n",
       "       [ -51.84977972, -392.17395257,  188.50798593, ...,   18.5109603 ,\n",
       "         -75.96611653,   -7.67736302],\n",
       "       ...,\n",
       "       [-178.0534095 , -160.07838721,  257.61233558, ...,  -57.3811145 ,\n",
       "           6.70673288,  -54.26797595],\n",
       "       [ 130.60654125,    5.59174593, -513.85834969, ...,  -22.43044205,\n",
       "          12.51568244,  -36.3004746 ],\n",
       "       [-173.43566358,   24.71937319, -556.01892138, ...,  -48.33215133,\n",
       "          19.2936437 ,  -30.58306681]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import IncrementalPCA\n",
    "n_batches = 100\n",
    "inc_pca = IncrementalPCA(n_components=154)\n",
    "for X_batch in np.array_split(X_train, n_batches):\n",
    "    inc_pca.partial_fit(X_batch)\n",
    "X_reduced = inc_pca.transform(X_train)\n",
    "X_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"my_mnist.mmap\"\n",
    "X_mmap = np.memmap(filename, dtype='float32', mode='write', shape=X_train.shape)\n",
    "X_mmap[:] = X_train\n",
    "X_mmap.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_mmap = np.memmap(filename, dtype='float32', mode='readonly').reshape(-1,784)\n",
    "batch_size = X_mmap.shape[0] #n_batches\n",
    "inc_pca = IncrementalPCA(n_components=154, batch_size=batch_size)\n",
    "inc_pca.fit(X_mmap)\n",
    "inc_pca.batch_size_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_mmap.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dimensions: 100000000\n",
      "New dimensions: 7300\n"
     ]
    }
   ],
   "source": [
    "from sklearn.random_projection import johnson_lindenstrauss_min_dim\n",
    "\n",
    "m, epsilon = 5_000, 0.1 #Where m is the number of features and epsilon is our tolerance of information loss\n",
    "\n",
    "#make dimensions d be the number of dimenstions is possible to reducce\n",
    "d = johnson_lindenstrauss_min_dim(m, eps=epsilon)\n",
    "\n",
    "#show dimensionality before and after using 20,000 features\n",
    "print(f'Original dimensions: {m*20_000}')\n",
    "print(f'New dimensions: {d}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#example to show this reduction with a random gaussian distributed matrix\n",
    "\n",
    "#create number of features n\n",
    "n = 20_000\n",
    "\n",
    "#create random seed and random matrix P\n",
    "np.random.seed(42)\n",
    "P = np.random.rand(d, n) / np.sqrt(d) #std dev = square root of variance\n",
    "\n",
    "#now create a fake data set that will be matrix multiplied by the P matrix to a reduced matrix\n",
    "\n",
    "X = np.random.randn(m,n)\n",
    "X_reduced = X @ P.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old dimensions: (5000, 20000)\n",
      "New dimensions(5000, 7300)\n"
     ]
    }
   ],
   "source": [
    "print(f'Old dimensions: {X.shape}\\nNew dimensions{X_reduced.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 7300)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#another way we can perfrom the cell above is using GaussianRandomProjection class\n",
    "from sklearn.random_projection import GaussianRandomProjection\n",
    "gaussian_rand_proj = GaussianRandomProjection(eps=epsilon, random_state=42)\n",
    "X_reduced = gaussian_rand_proj.fit_transform(X)\n",
    "#We'll just print the shape to show that change since we already know X is (5000, 20,000)\n",
    "X_reduced.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the class <span style='color: green; text-decoration: underline'>SparseRandomProjection</span> to get the same dimension reduction with having to do all that processing. It will make a sparse matrix so it uses less data and reduce its dimensionality as well with faster processing times. Use this class before <span style='color: green; text-decoration: underline'>GaussianRandomProjection</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This next part deals with LLE or Locally linear embedding. It does not rely on projection and is similar to nearest neighbors\n",
    "as it kinda uses it for nearest neighbors\n",
    "\"\"\"\n",
    "from sklearn.datasets import make_swiss_roll\n",
    "from sklearn.manifold import LocallyLinearEmbedding\n",
    "\n",
    "#import swiss roll in X and y and initiallize lle with n_components and n_neighbors, with a random state of 42\n",
    "swiss_X, y = make_swiss_roll(n_samples=1000, noise=0.2, random_state=42)\n",
    "lle = LocallyLinearEmbedding(n_components=2, n_neighbors=10, random_state=42)\n",
    "\n",
    "#convert X to a 2D array using LLE\n",
    "X_unrolled = lle.fit_transform(swiss_X)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ca9c90c9b299e3c35d28bc96236d8f2c0bd3d51256cb5ad616950692d4a1a879"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
